\section{Introduction} 
The main purpose of the present work is to show how to estimate the Hurst exponent for a mono-dimensional time series, using the Detrended Fluctuation Analysis and how to interpret it. \\
This technique is developed as a C program, which can be found online\cite{Foradori}, and applied to a time series about sunspots. 

\subsection{Hurst Exponent}
The Hurst exponent is a dimensionless estimator used to evaluate self-similarity and long-range dependence properties of time series. \\
It was introduced by Harold Edwin Hurst to describe the regularities of Nile water level. \\
He defined it as
\begin{equation}
\mathbb{E} \left[\frac{R(N)}{S(N)}\right] \propto N^{H} \quad \text{for } N \to \infty
\label{eq:defHE}
\end{equation}
where $H \in \left[0, 1 \right] $ is the Hurst exponent, $S (N)$ is the standard deviation and $R(N)$ is the range, the difference between maximum and minimum values of a given time series with $N$ points.

A modern definition is discussed in \autoref{sec:theory}.

\subsection{Detrended Fluctuation Analysis}
It is often not advisable to estimate the Hurst exponent of measured data by \autoref{eq:defHE}, because often they carry not interesting and explicit trends, or, more often, do not exhibit a clear scaling.

So it is useful to introduce a particular way of detrending, called Detrended Fluctuation Analysis (DFA), which was developed by Peng \etal\cite{PhysRevE.49.1685} for the analysis of DNA.

The advantages of DFA over other methods are that of permitting the detection of long-range correlations in time series with non-stationary and of avoiding the spurious detection of apparent long-range correlations that are an artefact of non-stationary.
